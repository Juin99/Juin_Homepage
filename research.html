<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Research</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Homepage</div>
<div class="menu-item"><a href="index.html">About&nbsp;Me</a></div>
<div class="menu-item"><a href="experiences.html">Experiences</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research</h1>
</div>
<div class="infoblock">
<div class="blockcontent">
<p>The goal of my researches is to develop efficient data-driven decision-making algorithms and theory, enabling principled approaches via statistics and machine learning to addressing modern societal, medical, and economic challenges. Specifically, Iâ€™m interested in the following perspectives and topics:</p>
<ul>
<li><p>Demystifying the mathematical and statistical foundations of machine learning/reinforcement learning algorithms.</p>
</li>
<li><p>Promoting robustness of machine learning methods in complex dynamic systems, with special attention to problems
involving distributional robustness, confounding, and partial observability.</p>
</li>
<li><p>Developing modern machine learning models with low computational and memorial costs.<br /></p>
</li>
</ul>
<p>With these goals, my researches are based across the span of statistics, optimization, and machine learning.</p>
</div></div>
<h2>Selected Research Experiences (By Topic)</h2>
<p>During my undergraduate years, I am quite fortunate to be advised by several wonderful professors, for which I can explore the frontiers of areas that I am interested in.</p>
<h3>Reinforcement Learning Theory</h3>
<table class="imgtable"><tr><td>
<img src="fig_econ.png" alt="alt text" width="300px" height="233.6" />&nbsp;</td>
<td align="left"><p><b>Project 1:</b> Reinforcement learning for Markovian exchange economy<br />
<b>Advisor:</b> Prof. <a href="https://scholar.google.com/citations?user=k7NgVSUAAAAJ&amp;hl=en&amp;citsig=AMD79orFe0lK27dXDHbcx97hUezW2OFKQQ">Zhuoran Yang</a> (Yale@Stat.&DS.), Prof. <a href="https://zhaoranwang.github.io">Zhaoran Wang</a> (NU@IEMS), Prof. <a href="https://people.eecs.berkeley.edu/~jordan/">Michael I. Jordan</a> (UC Berkeley@EECS)<br /></p>
<ul>
<li><p>We propose and study a bilevel economic system, known as a <b>M</b>arkov <b>E</b>xchange <b>E</b>conomy (MEE), from the viewpoint of multi-agent reinforcement learning.
With a central planner and a group of agents, the system is optimized when the agents achieve competitive equilibrium (CE) and the planner steers the economy to social welfare maximization (SWM).</p>
</li>
<li><p>We derived a novel single metric to mathematically characterize such an optimality, based on which we design provably efficient online and offline learning algorithms (MOLM/MPLM) for solving the economy.
Our algorithm can readily incorporate general function approximation tools for handling large state spaces and achieves optimal online regret/offline suboptimality.</p>
</li>
<li><p>Our <a href="https://proceedings.mlr.press/v162/liu22l.html">work</a> is presented as a poster at ICML 2022.</p>
</li>
</ul>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="fig_pomdp.png" alt="alt text" width="300px" height="165.5" />&nbsp;</td>
<td align="left"><p><b>Project 2:</b> Offline policy optimization in partially observable Markov decision processes<br />
<b>Advisor:</b> Prof. <a href="https://scholar.google.com/citations?user=k7NgVSUAAAAJ&amp;hl=en&amp;citsig=AMD79orFe0lK27dXDHbcx97hUezW2OFKQQ">Zhuoran Yang</a> (Yale), Prof. <a href="https://zhaoranwang.github.io">Zhaoran Wang</a> (NU)<br /></p>
<ul>
<li><p>We propose the <i>first</i> provably efficient offline RL algorithm known as P3O for POMDPs with a <i>confounded dataset</i>.</p>
</li>
<li><p>At the core of P3O is a coupled sequence of pessimistic confidence regions constructed via proximal causal inference, which is formulated as minimax estimation.
We also derived novel theoretical analysis techniques to show the fast statistical rate for the confidence region of this kind, allowing us to prove -1/2-subobtimality rate of P3O.</p>
</li>
<li><p>Our <a href="https://arxiv.org/abs/2205.13589">work</a> is on arXiv and is presented at INFORMS2022 and Stat.&ML workshop at University of Michigan.</p>
</li>
</ul>
</td></tr></table>
<h3>Robust Reinforcement Learning </h3>
<table class="imgtable"><tr><td>
<img src="fig_scmdp.png" alt="alt text" width="300px" height="202.5" />&nbsp;</td>
<td align="left"><p><b>Project:</b> Learning robust policy against disturbance in transition dynamics<br />
<b>Advisor:</b> Prof. <a href="https://miralab.ai/people/jie-wang/">Jie Wang</a> (USTC@EEIS)<br /></p>
<ul>
<li><p>We propose a <b>S</b>tate-<b>C</b>onservative <b>M</b>arkov <b>D</b>ecision <b>P</b>rocess (SC-MDP) to learn robust policies against disturbance in transition dynamics without task-specific knowledge of the disturbance. 
From the theoretical perspective, in the tabular setting, we design the State-Conservative Policy Iteration algorithm (SCPI) to learn the correponding optimal policy in a SC-MDP, which enjoys convergence guarantees.</p>
</li>
<li><p>To promote robustness in continuous control tasks, we further propose the State-Conservative Policy Optimization algorithm (SCPO) based on SCPI, and we efficiently implement it via a Gradient Based Regularizer (GBR). 
Mujoco experiments show that SCPO can improve robustness against disturbance in transition dynamics in various domains.		</p>
</li>
<li><p>Our <a href="https://arxiv.org/abs/2112.10513">work</a> is presented at AAAI 2022.</p>
</li>
</ul>
</td></tr></table>
<h3>Machine Learning Model Compression</h3>
<table class="imgtable"><tr><td>
<img src="fig_sfw.png" alt="alt text" width="300px" height="229.1" />&nbsp;</td>
<td align="left"><p><b>Project:</b> Network pruning via Stochastic Frank-Wolfe: any sparsity and no retraining<br />
<b>Advisor:</b> Prof. <a href="https://express.adobe.com/page/CAdrFMJ9QeI2y/">Zhangyang (Atlas) Wang</a> (UT Austin@ECE)<br /></p>
<ul>
<li><p>We propose Stochastic Frank-Wolfe Pruning (SFW-pruning), a one-shot unstructured deep neural network pruning algorithm. 
It results in consistent and competitive model performance for varying pruning ratios <i>without retraining</i>.</p>
</li>
<li><p>We customize a meta-learning-based initialization scheme for SFW-based DNN training, leading to more consistent and competitive performance under more varying pruning ratios. </p>
</li>
<li><p>We demonstrate the efficiency and competitiveness of SFW-pruning over various DNN models and CV datasets.</p>
</li>
<li><p>Our <a href="https://openreview.net/forum?id=O1DEtITim__">work</a> is a <b>spotlight</b> presentation at ICLR 2022.</p>
</li>
</ul>
</td></tr></table>
<div id="footer">
<div id="footer-text">
Copyright 2022 Miao Lu. Page generated 2022-11-03 02:41:56 CST.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
